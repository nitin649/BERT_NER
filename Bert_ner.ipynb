{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Bert_ner.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyO3MUM+2UNXg5FuwVvFAQLw",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/nitin649/BERT_NER/blob/main/Bert_ner.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dCm3_BQP73Gs",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "f7386c2e-c817-4026-a4c1-d73b13bc79b4"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y-fQ1mMItSCV",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 356
        },
        "outputId": "b8cfff3c-5be3-4606-d623-8910fc8c8069"
      },
      "source": [
        "!pip install transformers"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.6/dist-packages (3.3.1)\n",
            "Requirement already satisfied: tokenizers==0.8.1.rc2 in /usr/local/lib/python3.6/dist-packages (from transformers) (0.8.1rc2)\n",
            "Requirement already satisfied: sacremoses in /usr/local/lib/python3.6/dist-packages (from transformers) (0.0.43)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.6/dist-packages (from transformers) (3.0.12)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from transformers) (1.18.5)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.6/dist-packages (from transformers) (2019.12.20)\n",
            "Requirement already satisfied: dataclasses; python_version < \"3.7\" in /usr/local/lib/python3.6/dist-packages (from transformers) (0.7)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from transformers) (2.23.0)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.6/dist-packages (from transformers) (4.41.1)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.6/dist-packages (from transformers) (20.4)\n",
            "Requirement already satisfied: sentencepiece!=0.1.92 in /usr/local/lib/python3.6/dist-packages (from transformers) (0.1.91)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (0.16.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (7.1.2)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (1.15.0)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2020.6.20)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from packaging->transformers) (2.4.7)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VL9QmZOus8UR"
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn \n",
        "import os \n",
        "import pandas as pd\n",
        "from sklearn import model_selection\n",
        "from transformers import BertTokenizer\n",
        "from transformers import BertModel\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset\n",
        "from torch.utils.data import DataLoader"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tNOKj_CLGFL7"
      },
      "source": [
        "def fun_create_data():\n",
        "  with open(r'/content/drive/My Drive/Colab Notebooks/NamesDataset/train.txt','r') as file:\n",
        "    x=file.readlines()\n",
        "   \n",
        "    new_line_index=[]\n",
        "    final_sents=[]\n",
        "    original_tags=[]\n",
        "    encoded_tags=[]\n",
        "    tags_dict={}\n",
        "    start=0\n",
        "    count=-1\n",
        "    tag_index=-1\n",
        "    for i in x:\n",
        "        count+=1\n",
        "        if len(i)==1:\n",
        "            new_line_index.append(count)\n",
        "    for end in new_line_index[:5000]:\n",
        "        sent=' '.join([word.split(' ')[0] for word in x[start:end]])\n",
        "        tag=','.join([word.split(' ')[-1].replace('\\n',' ') for word in x[start:end]])\n",
        "        final_sents.append(sent.split(' '))\n",
        "        original_tags.append(tag.split(','))\n",
        "        for single_tag in tag.split(','):\n",
        "          if single_tag not in tags_dict:\n",
        "            tag_index+=1\n",
        "            tags_dict[single_tag]=tag_index\n",
        "\n",
        "        start=end+1\n",
        "    print(tags_dict)\n",
        "    for i in original_tags:\n",
        "       new_tags=[]\n",
        "       for j in i:\n",
        "         new_tags.append(tags_dict[j])\n",
        "       encoded_tags.append(new_tags)\n",
        "\n",
        "    \n",
        "\n",
        "    print(len(final_sents),len(original_tags),len(encoded_tags),tags_dict)\n",
        "    return final_sents,original_tags,encoded_tags,len(tags_dict),tags_dict"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8wNeygLtuKqN",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        },
        "outputId": "34a43515-27a5-4f34-e983-fd8751c90d9a"
      },
      "source": [
        "final_sents,original_tags,encoded_tags,num_classes,tags_dict=fun_create_data()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'B-ORG ': 0, 'O ': 1, 'B-MISC ': 2, 'B-PER ': 3, 'I-PER ': 4, 'B-LOC ': 5, 'I-ORG ': 6, 'I-MISC ': 7, 'I-LOC ': 8}\n",
            "5000 5000 5000 {'B-ORG ': 0, 'O ': 1, 'B-MISC ': 2, 'B-PER ': 3, 'I-PER ': 4, 'B-LOC ': 5, 'I-ORG ': 6, 'I-MISC ': 7, 'I-LOC ': 8}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tgMoPG-_6IiJ",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "f89b396e-9871-4d9d-e60e-e37bcb341fcb"
      },
      "source": [
        "print(tags_dict)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'B-ORG ': 0, 'O ': 1, 'B-MISC ': 2, 'B-PER ': 3, 'I-PER ': 4, 'B-LOC ': 5, 'I-ORG ': 6, 'I-MISC ': 7, 'I-LOC ': 8}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kErTUT8Iyv0C",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 67
        },
        "outputId": "48604d5f-601f-4e16-bbd5-f9a030c73681"
      },
      "source": [
        "print(len(final_sents))\n",
        "print(len(original_tags))\n",
        "print(len(encoded_tags))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "5000\n",
            "5000\n",
            "5000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XXI8kZv4LgO5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 454
        },
        "outputId": "29d290da-e758-47ae-b74f-b30737954cbe"
      },
      "source": [
        "final_sents[2450]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['American',\n",
              " 'Dennis',\n",
              " 'Mitchell',\n",
              " 'upstaged',\n",
              " 'a',\n",
              " 'trio',\n",
              " 'of',\n",
              " 'past',\n",
              " 'and',\n",
              " 'present',\n",
              " 'Olympic',\n",
              " '100',\n",
              " 'metres',\n",
              " 'champions',\n",
              " 'on',\n",
              " 'Friday',\n",
              " 'with',\n",
              " 'a',\n",
              " 'storming',\n",
              " 'victory',\n",
              " 'at',\n",
              " 'the',\n",
              " 'Brussels',\n",
              " 'grand',\n",
              " 'prix',\n",
              " '.']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 49
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZZ9sDQeVLjsA",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 454
        },
        "outputId": "f67aa4a4-687d-4d87-f60f-bb491dc65002"
      },
      "source": [
        "original_tags[2450]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['B-MISC ',\n",
              " 'B-PER ',\n",
              " 'I-PER ',\n",
              " 'O ',\n",
              " 'O ',\n",
              " 'O ',\n",
              " 'O ',\n",
              " 'O ',\n",
              " 'O ',\n",
              " 'O ',\n",
              " 'B-MISC ',\n",
              " 'O ',\n",
              " 'O ',\n",
              " 'O ',\n",
              " 'O ',\n",
              " 'O ',\n",
              " 'O ',\n",
              " 'O ',\n",
              " 'O ',\n",
              " 'O ',\n",
              " 'O ',\n",
              " 'O ',\n",
              " 'B-LOC ',\n",
              " 'O ',\n",
              " 'O ',\n",
              " 'O ']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 48
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-KQPeBoUMADj",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "ea3009ec-d749-4d3c-c489-ba1ebd07791a"
      },
      "source": [
        "encoded_tags[2450]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[2, 3, 4, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 5, 1, 1, 1]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 50
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HquaUzwaMFAD",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 168
        },
        "outputId": "1196ca9b-f090-4b57-bbc1-2aaae4b7cf34"
      },
      "source": [
        "tags_dict"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'B-LOC ': 5,\n",
              " 'B-MISC ': 2,\n",
              " 'B-ORG ': 0,\n",
              " 'B-PER ': 3,\n",
              " 'I-LOC ': 8,\n",
              " 'I-MISC ': 7,\n",
              " 'I-ORG ': 6,\n",
              " 'I-PER ': 4,\n",
              " 'O ': 1}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 51
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VhMJGSzETdj-"
      },
      "source": [
        "\n",
        "def max_sent_len(final_sents):\n",
        "  max_sent=[]\n",
        "  sent=[]\n",
        "  for i in final_sents:\n",
        "    for j in i:\n",
        "      tokenizer=BertTokenizer.from_pretrained('bert-base-cased',do_lower_case=True)\n",
        "      x=tokenizer.tokenize(j)\n",
        "      sent.extend(x)\n",
        "    max_sent.append((len(sent)))\n",
        "  return max(max_sent)\n",
        "      "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FQBghwbnYwdE"
      },
      "source": [
        "#max_sent_len(final_sents=final_sents)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gcwAj-1ZuPXg"
      },
      "source": [
        "from sklearn import model_selection\n",
        "train_sents,test_sents,train_tags,test_tags=model_selection.train_test_split(final_sents,encoded_tags,random_state=42,test_size=0.1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "INWPNEkcv3Io",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 84
        },
        "outputId": "daefb11f-9f23-4c0d-827e-22b9079db230"
      },
      "source": [
        "print(len(train_sents))\n",
        "print(len(test_sents))\n",
        "print(len(train_tags))\n",
        "print(len(test_tags))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "4500\n",
            "500\n",
            "4500\n",
            "500\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wrkpZHeHsfEF",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "5dd7eecf-5568-47f4-b8b9-b8386da18620"
      },
      "source": [
        "len(tags_dict)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "9"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QNo7MUgZwJpa"
      },
      "source": [
        "device=torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ymFl49hcyUDh"
      },
      "source": [
        "class NerDataset(Dataset):\n",
        "    def __init__(self,sents,tags,max_len):\n",
        "      #print(sents)\n",
        "      #print(tags)\n",
        "      self.sents=sents\n",
        "      self.tags=tags\n",
        "      self.max_len=max_len\n",
        "      #self.device=device\n",
        "      \n",
        "    def __len__(self):\n",
        "      return (len(self.sents))\n",
        "\n",
        "    def __getitem__(self,index):\n",
        "      self.tokenizer=BertTokenizer.from_pretrained('bert-base-cased',do_lower_case=True)\n",
        "\n",
        "      sent=self.sents[index]\n",
        "      tag=self.tags[index]\n",
        "\n",
        "      final_sents=[]\n",
        "      final_tags=[]\n",
        "      \n",
        "      for i , s in enumerate(sent):\n",
        "          tokenize_words=self.tokenizer.encode(\n",
        "              s,\n",
        "              add_special_tokens=False\n",
        "          )\n",
        "          input_len=len(tokenize_words)\n",
        "          final_sents.extend(tokenize_words)\n",
        "          final_tags.extend([tag[i]]*input_len)\n",
        "      \n",
        "      final_sents=final_sents[:self.max_len-2] #truncating\n",
        "      final_tags=final_tags[:self.max_len-2]\n",
        "\n",
        "      final_sents=[101] + final_sents + [102]\n",
        "      final_tags=[0] + final_tags+[0]\n",
        "\n",
        "      mask=[1] * len(final_sents)\n",
        "\n",
        "      segment_ids=[0]*len(final_sents)\n",
        "      \n",
        "\n",
        "      #padding our data\n",
        "\n",
        "      padding_len=self.max_len - len(final_sents)\n",
        "      #print(padding_len)\n",
        "      final_sents=final_sents +([0] * padding_len)\n",
        "      final_tags=final_tags +([0] * padding_len)\n",
        "      mask=mask+([0] * padding_len)\n",
        "      segment_ids=segment_ids + ([0] * padding_len)\n",
        "\n",
        "      #print(final_sents)\n",
        "      #print(len(final_tags))\n",
        "      #print(len(mask))\n",
        "      #print(len(segment_ids))\n",
        "      \n",
        "      return {\n",
        "          'id':torch.tensor(final_sents,dtype=torch.long),\n",
        "          'target_tag':torch.tensor(final_tags,dtype=torch.long),\n",
        "          'mask':torch.tensor(mask,dtype=torch.long),\n",
        "          'token_type_ids':torch.tensor(segment_ids,dtype=torch.long)\n",
        "      }\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VyHnJRx09TeA"
      },
      "source": [
        "max_sent_len=128\n",
        "train_dataset=NerDataset(sents=train_sents,tags=train_tags,max_len=max_sent_len)\n",
        "test_dataset=NerDataset(sents=test_sents,tags=test_tags,max_len=max_sent_len)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sAStjXauw00-",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "2a4f4790-c051-43e3-c0eb-c6be0f7c201e"
      },
      "source": [
        "128*5"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "640"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BtTV5_-7qa1b"
      },
      "source": [
        "def save_checkpoint(state,filename=\"/content/drive/My Drive/Colab Notebooks/TransformerMachineTranslation/bert_ner_checkpoint/bert_checkpoint_second.pth\"):\n",
        "  print('saving checkpoint')\n",
        "  torch.save(state,filename)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "soRl3rZKDyeM"
      },
      "source": [
        "def load_checkpoint(checkpoint):\n",
        "  print('loading checkpoint')\n",
        "  model.load_state_dict(checkpoint['state_dict'])\n",
        "  optimizer.load_state_dict(checkpoint['optimizer'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YyiHXAGnE_zU"
      },
      "source": [
        "load_model=False #whenever we have to load set this flag to true\n",
        "if load_model:\n",
        "  load_checkpoint(torch.load('/content/drive/My Drive/Colab Notebooks/TransformerMachineTranslation/bert_ner_checkpoint/bert_checkpoint_second.pth'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FRF0yNcRzdK9"
      },
      "source": [
        "def loss_fn(output, target, mask, num_labels):\n",
        "    lfn = nn.CrossEntropyLoss()\n",
        "    active_loss = mask.view(-1) == 1 #this  is used just for calculate loss just considering the original data not the padded one\n",
        "    #output shapes [batch,entries,classes]\n",
        "    active_logits = output.view(-1, num_labels) #converting into [entries , classes]\n",
        "    active_labels = torch.where(\n",
        "        active_loss,\n",
        "        target.view(-1),\n",
        "        torch.tensor(lfn.ignore_index).type_as(target)\n",
        "    )\n",
        "    #print(active_logits.size(),active_labels.size())\n",
        "    loss = lfn(active_logits, active_labels)\n",
        "    return loss\n",
        "\n",
        "class Model(nn.Module):\n",
        "    def __init__(self,num_tags,hidden_size,input_size,num_layer):\n",
        "      super().__init__()\n",
        "      self.num_tags=num_tags\n",
        "      self.bert = BertModel.from_pretrained('bert-base-cased')\n",
        "      self.lstm=nn.LSTM(input_size,hidden_size=hidden_size,num_layers=num_layer,batch_first=True,bidirectional=True)\n",
        "      self.hidden_size=hidden_size\n",
        "      self.num_layer=num_layer\n",
        "      self.bert_drop_1 = nn.Dropout(0.3)\n",
        "      self.out_tag = nn.Linear(hidden_size * 2 , self.num_tags)\n",
        "    \n",
        "    def forward(self,id,mask,token_type_ids,target_tag):\n",
        "      o1, _ = self.bert(id, attention_mask=mask, token_type_ids=token_type_ids)\n",
        "      o1 = self.bert_drop_1(o1)\n",
        "      h0=torch.zeros(self.num_layer*2,o1.size(0),self.hidden_size).to(device) #hidden state\n",
        "      c0=torch.zeros(self.num_layer*2,o1.size(0),self.hidden_size).to(device) #hidden state\n",
        "      o1,_=self.lstm(o1,(h0,c0))\n",
        "      #o1=torch.reshape(o1,[-1,self.hidden_size*2])\n",
        "      \n",
        "      tag = self.out_tag(o1)\n",
        "      loss_tag = loss_fn(tag, target_tag, mask, self.num_tags)\n",
        "\n",
        "      #print('output tags',tag)\n",
        "      return tag,loss_tag"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RSK0-3QMDQWf"
      },
      "source": [
        "class Model_Normal(nn.Module):\n",
        "    def __init__(self,num_tags):\n",
        "      super().__init__()\n",
        "      self.num_tags=num_tags\n",
        "      self.bert = BertModel.from_pretrained('bert-base-cased')\n",
        "      self.hidden_size=hidden_size\n",
        "      self.bert_drop_1 = nn.Dropout(0.3)\n",
        "      \n",
        "      self.out_tag = nn.Linear(768 , self.num_tags)\n",
        "    \n",
        "    def forward(self,id,mask,token_type_ids,target_tag):\n",
        "      o1, _ = self.bert(id, attention_mask=mask, token_type_ids=token_type_ids)\n",
        "      o1 = self.bert_drop_1(o1)\n",
        "\n",
        "      tag = self.out_tag(o1)\n",
        "      loss_tag = loss_fn(tag, target_tag, mask, self.num_tags)\n",
        "\n",
        "      #print('output tags',tag)\n",
        "      return tag,loss_tag"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9cOsfngc6TCV"
      },
      "source": [
        "#hyperparameter\n",
        "num_class =9\n",
        "hidden_size=256\n",
        "input_size=768\n",
        "batch_size=32\n",
        "num_layer=1\n",
        "num_epocs=3\n",
        "learning_rate=3e-5"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rFDByPWzuA4k"
      },
      "source": [
        "#model=Model(hidden_size=hidden_size,input_size=input_size,num_layer=num_layer,num_tags=num_class).to(device)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DsxHK2ronxNg"
      },
      "source": [
        "train_loader=DataLoader(dataset=train_dataset,batch_size=batch_size,shuffle=True)\n",
        "test_loader=DataLoader(dataset=test_dataset,batch_size=batch_size)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "31wU4Y2kn1cM"
      },
      "source": [
        "model=Model_Normal(num_tags=num_class).to(device)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7y5OCXhc617C"
      },
      "source": [
        "criterion=nn.CrossEntropyLoss()\n",
        "optimizer=optim.Adam(model.parameters(),lr=learning_rate)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DXfwtSsTIwxz"
      },
      "source": [
        "#criterion2=nn.CrossEntropyLoss()\n",
        "#optimizer2=optim.Adam(model2.parameters(),lr=learning_rate)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v8fm4jvT1keP"
      },
      "source": [
        "#for batch_idx , data in enumerate(train_loader):\n",
        "  #print(batch_idx , data)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R9AlZB1A7dzL"
      },
      "source": [
        "#for epoch in range(10):\n",
        "  #losses=[] #use for calculating loss at each epoch\n",
        "  #model.train()\n",
        "\n",
        "  #print(f'epoch number {epoch}')\n",
        "  #checkpoint={'state_dict':model.state_dict(),'optimizer':optimizer.state_dict()}\n",
        "  #save_checkpoint(checkpoint)\n",
        "  #for batch_idx , data in enumerate(train_loader):\n",
        "    #for k, v in data.items():\n",
        "        #data[k] = v.to(device)\n",
        "    #target_tag=data['target_tag']  \n",
        "    #forward\n",
        "    #_,loss=model(**data)\n",
        "    #print(scores.size())\n",
        "    #target_tag=target_tag.view(-1)\n",
        "    #loss=criterion(scores,target_tag)\n",
        "\n",
        "    #losses.append(loss.item())\n",
        "\n",
        "    #backward\n",
        "    #optimizer.zero_grad()\n",
        "    #loss.backward()\n",
        "\n",
        "    #optimizer.step()\n",
        "\n",
        "  #mean_loss=sum(losses)/len(losses)\n",
        "  #print(f\"loss after epoch number {epoch} is {mean_loss}\")  "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jeXEUs3aEOUV",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 521
        },
        "outputId": "e24a0f1d-70e0-419d-9932-baaf55f44aec"
      },
      "source": [
        "for epoch in range(10):\n",
        "  print(f'epoch number {epoch}')\n",
        "  losses=[] #use for calculating loss at each epoch\n",
        "  #model.train()\n",
        "\n",
        " \n",
        "  checkpoint={'state_dict':model.state_dict(),'optimizer':optimizer.state_dict()}\n",
        "  save_checkpoint(checkpoint)\n",
        "  for batch_idx , data in enumerate(train_loader):\n",
        "    for k, v in data.items():\n",
        "        data[k] = v.to(device)\n",
        "    target_tag=data['target_tag']  \n",
        "    #forward\n",
        "    _,loss=model(**data)\n",
        "    #print(scores.size())\n",
        "    #target_tag=target_tag.view(-1)\n",
        "    #loss=criterion2(scores,target_tag)\n",
        "\n",
        "    losses.append(loss.item())\n",
        "\n",
        "    #backward\n",
        "    optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "\n",
        "    optimizer.step()\n",
        "\n",
        "  mean_loss=sum(losses)/len(losses)\n",
        "  print(f\"loss after epoch number {epoch} is {mean_loss}\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "epoch number 0\n",
            "saving checkpoint\n",
            "loss after epoch number 0 is 0.44657372508911375\n",
            "epoch number 1\n",
            "saving checkpoint\n",
            "loss after epoch number 1 is 0.11070421641917093\n",
            "epoch number 2\n",
            "saving checkpoint\n",
            "loss after epoch number 2 is 0.05248481154124787\n",
            "epoch number 3\n",
            "saving checkpoint\n",
            "loss after epoch number 3 is 0.025369142778283844\n",
            "epoch number 4\n",
            "saving checkpoint\n",
            "loss after epoch number 4 is 0.020110411167389167\n",
            "epoch number 5\n",
            "saving checkpoint\n",
            "loss after epoch number 5 is 0.012990778566384358\n",
            "epoch number 6\n",
            "saving checkpoint\n",
            "loss after epoch number 6 is 0.007952099057502306\n",
            "epoch number 7\n",
            "saving checkpoint\n",
            "loss after epoch number 7 is 0.007720789418420391\n",
            "epoch number 8\n",
            "saving checkpoint\n",
            "loss after epoch number 8 is 0.007263534914550259\n",
            "epoch number 9\n",
            "saving checkpoint\n",
            "loss after epoch number 9 is 0.005139967839477967\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kKMXRy_RBv_h",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 168
        },
        "outputId": "98dc3873-ed34-45ca-d8d2-ecd50b53d498"
      },
      "source": [
        "/tags_dict"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'B-LOC ': 5,\n",
              " 'B-MISC ': 2,\n",
              " 'B-ORG ': 0,\n",
              " 'B-PER ': 3,\n",
              " 'I-LOC ': 8,\n",
              " 'I-MISC ': 7,\n",
              " 'I-ORG ': 6,\n",
              " 'I-PER ': 4,\n",
              " 'O ': 1}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 91
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1BFPqWTosw0O",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 709
        },
        "outputId": "9e01c415-3c64-4ace-8a8e-5950ca02f1d5"
      },
      "source": [
        "model.eval()\n",
        "tokenizer=BertTokenizer.from_pretrained('bert-base-cased',do_lower_case=True)\n",
        "with torch.no_grad():\n",
        "  list_word=[]\n",
        "  tokenize_word=[]\n",
        "  sent='Robert is good boy and he is going to London and working in Google'\n",
        "  for word in sent.split(' '):\n",
        "      tokenize_word.extend(tokenizer.tokenize(word))\n",
        "  tokenize_word=['cls']+tokenize_word+['sep']\n",
        "  print(f\"tokenize words {tokenize_word}\")\n",
        "  for word in sent.split(' '):\n",
        "    list_word.append(word)\n",
        "  test_dataset=NerDataset(sents=[list_word],tags=[[0]*len(list_word)],max_len=128)\n",
        "  data=test_dataset[0] #only single element\n",
        "  print(data)\n",
        "  for k , v in data.items():\n",
        "      data[k]=v.to(device).unsqueeze(0) #to add axis to make input like this for bert [1,128] single sent batch\n",
        "      #print('final_data',data)\n",
        "  tag , _  = model(**data)\n",
        "  #print(tokenize_sent)\n",
        "  print(tag.size())\n",
        "  print(tag)\n",
        "  values=tag.argmax(2).view(-1)\n",
        "  new_val=values.to('cpu').numpy() #this is use for printing on cpu not on cuda \n",
        "  final_values=[]\n",
        "  for i in range(len(tokenize_word)):\n",
        "      final_values.append(new_val[i])\n",
        "  print(f'tokenize sent {tokenize_word}')\n",
        "  print(f'tags for tokenize word {final_values}')\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tokenize words ['cls', 'robe', '##rt', 'is', 'good', 'boy', 'and', 'he', 'is', 'going', 'to', 'lo', '##ndon', 'and', 'working', 'in', 'go', '##og', '##le', 'sep']\n",
            "{'id': tensor([  101, 11580,  3740,  1110,  1363,  2298,  1105,  1119,  1110,  1280,\n",
            "         1106, 25338, 17996,  1105,  1684,  1107,  1301,  8032,  1513,   102,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0]), 'target_tag': tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0, 0, 0, 0, 0, 0, 0]), 'mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0,\n",
            "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0, 0, 0, 0, 0, 0, 0]), 'token_type_ids': tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0, 0, 0, 0, 0, 0, 0])}\n",
            "torch.Size([1, 128, 9])\n",
            "tensor([[[10.3367, -0.6317, -1.0668,  ..., -0.5620, -1.5661, -1.6004],\n",
            "         [ 0.1364, -0.9688, -0.9801,  ..., -1.2976, -1.5360, -1.1066],\n",
            "         [-0.2733, -0.3113, -1.2621,  ..., -0.9427, -1.6348, -1.2493],\n",
            "         ...,\n",
            "         [-0.4229,  8.3900, -0.8781,  ..., -1.8328, -1.9485, -1.2936],\n",
            "         [-0.9169, -1.1362, -1.1757,  ..., -1.8899, -1.2978, -1.1399],\n",
            "         [-0.7750, -0.5857, -0.9783,  ..., -2.0133, -1.5449, -1.3505]]],\n",
            "       device='cuda:0')\n",
            "tokenize sent ['cls', 'robe', '##rt', 'is', 'good', 'boy', 'and', 'he', 'is', 'going', 'to', 'lo', '##ndon', 'and', 'working', 'in', 'go', '##og', '##le', 'sep']\n",
            "tags for tokenize word [0, 3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 5, 5, 1, 1, 1, 1, 1, 1, 0]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ucP9u2yMKJP7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 168
        },
        "outputId": "a78dd7e9-7665-482b-9e8e-741fac56c10a"
      },
      "source": [
        "tags_dict"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'B-LOC ': 5,\n",
              " 'B-MISC ': 2,\n",
              " 'B-ORG ': 0,\n",
              " 'B-PER ': 3,\n",
              " 'I-LOC ': 8,\n",
              " 'I-MISC ': 7,\n",
              " 'I-ORG ': 6,\n",
              " 'I-PER ': 4,\n",
              " 'O ': 1}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P8tpGff75wQ9"
      },
      "source": [
        "def real_time(sent):#this function can be used in real time\n",
        "    training_model=Model_Normal(num_tags=num_class).to(device)\n",
        "    checkpoint=torch.load('/content/drive/My Drive/Colab Notebooks/TransformerMachineTranslation/bert_ner_checkpoint/bert_checkpoint_second.pth')\n",
        "    training_model.load_state_dict(checkpoint['state_dict'])\n",
        "\n",
        "    tokenizer=BertTokenizer.from_pretrained('bert-base-cased',do_lower_case=True)\n",
        "    training_model.eval()\n",
        "    with torch.no_grad():\n",
        "      list_word=[]\n",
        "      tokenize_word=[]\n",
        "      #sent='Robert is good boy and he is going to London google'\n",
        "      for word in sent.split(' '):\n",
        "          tokenize_word.extend(tokenizer.tokenize(word))\n",
        "      tokenize_word=['cls']+tokenize_word+['sep']\n",
        "      print(f\"tokenize words {tokenize_word}\")\n",
        "      for word in sent.split(' '):\n",
        "        list_word.append(word)\n",
        "      test_dataset=NerDataset(sents=[list_word],tags=[[0]*len(list_word)],max_len=128)\n",
        "      data=test_dataset[0] #only single element\n",
        "      print(data)\n",
        "      for k , v in data.items():\n",
        "          data[k]=v.to(device).unsqueeze(0) #to add axis to make input like this for bert [1,128] single sent batch\n",
        "          #print('final_data',data)\n",
        "      tag , _  = training_model(**data)\n",
        "      #print(tokenize_sent)\n",
        "      print(tag.size())\n",
        "      print(tag)\n",
        "      values=tag.argmax(2).view(-1)\n",
        "      new_val=values.to('cpu').numpy() #this is use for printing on cpu not on cuda \n",
        "      final_values=[]\n",
        "      for i in range(len(tokenize_word)):\n",
        "          final_values.append(new_val[i])\n",
        "      print(f'tokenize sent {tokenize_word}')\n",
        "      print(f'tags for tokenize word {final_values}')\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OobW4ORi6cGk",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 709
        },
        "outputId": "ac3dc5c7-9f40-49fa-9617-bddeec88d9d1"
      },
      "source": [
        "real_time('Nitin is going America')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tokenize words ['cls', 'ni', '##tin', 'is', 'going', 'am', '##eric', '##a', 'sep']\n",
            "{'id': tensor([  101, 11437,  6105,  1110,  1280,  1821, 26237,  1161,   102,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0]), 'target_tag': tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0, 0, 0, 0, 0, 0, 0]), 'mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0, 0, 0, 0, 0, 0, 0]), 'token_type_ids': tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0, 0, 0, 0, 0, 0, 0])}\n",
            "torch.Size([1, 128, 9])\n",
            "tensor([[[10.3210, -1.1132, -0.7598,  ..., -1.0710, -1.9030, -1.7060],\n",
            "         [ 3.1173, -2.0027,  0.1031,  ..., -2.2036, -1.8273, -1.5899],\n",
            "         [ 2.9987, -1.1825,  0.0917,  ..., -2.4406, -2.3107, -2.5336],\n",
            "         ...,\n",
            "         [ 1.1784,  4.1246,  1.5017,  ..., -2.2658, -0.7901, -0.8195],\n",
            "         [ 0.9076,  5.0572,  0.8512,  ..., -2.1336, -0.7540, -1.0229],\n",
            "         [ 1.6377,  5.3447,  1.1813,  ..., -2.2297, -1.3698, -1.3885]]],\n",
            "       device='cuda:0')\n",
            "tokenize sent ['cls', 'ni', '##tin', 'is', 'going', 'am', '##eric', '##a', 'sep']\n",
            "tags for tokenize word [0, 3, 3, 1, 1, 5, 5, 5, 0]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "juhCHItCzILy",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 168
        },
        "outputId": "7473d154-1c7e-4c3f-a39d-e5287e873bd6"
      },
      "source": [
        "tags_dict"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'B-LOC ': 5,\n",
              " 'B-MISC ': 2,\n",
              " 'B-ORG ': 0,\n",
              " 'B-PER ': 3,\n",
              " 'I-LOC ': 8,\n",
              " 'I-MISC ': 7,\n",
              " 'I-ORG ': 6,\n",
              " 'I-PER ': 4,\n",
              " 'O ': 1}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 51
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nNBT6Ao-WXpk"
      },
      "source": [
        "#tokenizer=BertTokenizer.from_pretrained('bert-base-cased',do_lower_case=True)\n",
        "\n",
        "def real_time(sent,model):\n",
        "  tokenize_sent=[]\n",
        "  tokenizer=BertTokenizer.from_pretrained('bert-base-cased',do_lower_case=True)\n",
        "\n",
        "  device=torch.device('cuda')\n",
        "  #model=Model(num_tags=9,hidden_size=256,input_size=768)\n",
        "  checkpoint=torch.load('/content/drive/My Drive/Colab Notebooks/TransformerMachineTranslation/bert_ner_checkpoint/bert_checkpoint_second.pth')\n",
        "  model.load_state_dict(checkpoint['state_dict'])\n",
        "\n",
        "  model=model.to(device=device)\n",
        "  list_word=[]\n",
        "  words=sent.split(' ')\n",
        "  for word in words:\n",
        "    x=tokenizer.tokenize(word)\n",
        "    tokenize_sent.extend(x)\n",
        "  tokenize_sent=['CLS'] + tokenize_sent + ['SEP']\n",
        "  for word in words :\n",
        "    list_word.append(word)\n",
        "  test_dataset=NerDataset(sents=[list_word],tags=[[0]*len(list_word)],max_len=128)\n",
        "  model.eval()\n",
        "  \n",
        "  with torch.no_grad():\n",
        "    data=test_dataset[0] #only single element\n",
        "    for k , v in data.items():\n",
        "\n",
        "      data[k]=v.to(device).unsqueeze(0) #to add axis to make input like this for bert [1,128] single sent batch\n",
        "\n",
        "    print('final_data',data)\n",
        "    tag = model(**data)\n",
        "  \n",
        "    print(tokenize_sent)\n",
        "    print(tag.argmax(1))\n",
        "    #print(tag.size())\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "321yxSgrioTK"
      },
      "source": [
        "\n",
        "#tokenizer.tokenize('My name is nitin I live in India and work in India.Pvt Ltd')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fGRkxF9Et4Uf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "935fa603-8ae1-4819-c2f4-a1cc3a0b7be0"
      },
      "source": [
        "\n",
        "x=real_time('Nitin sharma is my name',model)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model(\n",
            "  (bert): BertModel(\n",
            "    (embeddings): BertEmbeddings(\n",
            "      (word_embeddings): Embedding(28996, 768, padding_idx=0)\n",
            "      (position_embeddings): Embedding(512, 768)\n",
            "      (token_type_embeddings): Embedding(2, 768)\n",
            "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "      (dropout): Dropout(p=0.1, inplace=False)\n",
            "    )\n",
            "    (encoder): BertEncoder(\n",
            "      (layer): ModuleList(\n",
            "        (0): BertLayer(\n",
            "          (attention): BertAttention(\n",
            "            (self): BertSelfAttention(\n",
            "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "            (output): BertSelfOutput(\n",
            "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (intermediate): BertIntermediate(\n",
            "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "          )\n",
            "          (output): BertOutput(\n",
            "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "            (dropout): Dropout(p=0.1, inplace=False)\n",
            "          )\n",
            "        )\n",
            "        (1): BertLayer(\n",
            "          (attention): BertAttention(\n",
            "            (self): BertSelfAttention(\n",
            "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "            (output): BertSelfOutput(\n",
            "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (intermediate): BertIntermediate(\n",
            "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "          )\n",
            "          (output): BertOutput(\n",
            "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "            (dropout): Dropout(p=0.1, inplace=False)\n",
            "          )\n",
            "        )\n",
            "        (2): BertLayer(\n",
            "          (attention): BertAttention(\n",
            "            (self): BertSelfAttention(\n",
            "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "            (output): BertSelfOutput(\n",
            "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (intermediate): BertIntermediate(\n",
            "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "          )\n",
            "          (output): BertOutput(\n",
            "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "            (dropout): Dropout(p=0.1, inplace=False)\n",
            "          )\n",
            "        )\n",
            "        (3): BertLayer(\n",
            "          (attention): BertAttention(\n",
            "            (self): BertSelfAttention(\n",
            "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "            (output): BertSelfOutput(\n",
            "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (intermediate): BertIntermediate(\n",
            "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "          )\n",
            "          (output): BertOutput(\n",
            "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "            (dropout): Dropout(p=0.1, inplace=False)\n",
            "          )\n",
            "        )\n",
            "        (4): BertLayer(\n",
            "          (attention): BertAttention(\n",
            "            (self): BertSelfAttention(\n",
            "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "            (output): BertSelfOutput(\n",
            "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (intermediate): BertIntermediate(\n",
            "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "          )\n",
            "          (output): BertOutput(\n",
            "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "            (dropout): Dropout(p=0.1, inplace=False)\n",
            "          )\n",
            "        )\n",
            "        (5): BertLayer(\n",
            "          (attention): BertAttention(\n",
            "            (self): BertSelfAttention(\n",
            "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "            (output): BertSelfOutput(\n",
            "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (intermediate): BertIntermediate(\n",
            "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "          )\n",
            "          (output): BertOutput(\n",
            "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "            (dropout): Dropout(p=0.1, inplace=False)\n",
            "          )\n",
            "        )\n",
            "        (6): BertLayer(\n",
            "          (attention): BertAttention(\n",
            "            (self): BertSelfAttention(\n",
            "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "            (output): BertSelfOutput(\n",
            "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (intermediate): BertIntermediate(\n",
            "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "          )\n",
            "          (output): BertOutput(\n",
            "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "            (dropout): Dropout(p=0.1, inplace=False)\n",
            "          )\n",
            "        )\n",
            "        (7): BertLayer(\n",
            "          (attention): BertAttention(\n",
            "            (self): BertSelfAttention(\n",
            "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "            (output): BertSelfOutput(\n",
            "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (intermediate): BertIntermediate(\n",
            "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "          )\n",
            "          (output): BertOutput(\n",
            "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "            (dropout): Dropout(p=0.1, inplace=False)\n",
            "          )\n",
            "        )\n",
            "        (8): BertLayer(\n",
            "          (attention): BertAttention(\n",
            "            (self): BertSelfAttention(\n",
            "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "            (output): BertSelfOutput(\n",
            "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (intermediate): BertIntermediate(\n",
            "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "          )\n",
            "          (output): BertOutput(\n",
            "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "            (dropout): Dropout(p=0.1, inplace=False)\n",
            "          )\n",
            "        )\n",
            "        (9): BertLayer(\n",
            "          (attention): BertAttention(\n",
            "            (self): BertSelfAttention(\n",
            "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "            (output): BertSelfOutput(\n",
            "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (intermediate): BertIntermediate(\n",
            "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "          )\n",
            "          (output): BertOutput(\n",
            "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "            (dropout): Dropout(p=0.1, inplace=False)\n",
            "          )\n",
            "        )\n",
            "        (10): BertLayer(\n",
            "          (attention): BertAttention(\n",
            "            (self): BertSelfAttention(\n",
            "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "            (output): BertSelfOutput(\n",
            "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (intermediate): BertIntermediate(\n",
            "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "          )\n",
            "          (output): BertOutput(\n",
            "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "            (dropout): Dropout(p=0.1, inplace=False)\n",
            "          )\n",
            "        )\n",
            "        (11): BertLayer(\n",
            "          (attention): BertAttention(\n",
            "            (self): BertSelfAttention(\n",
            "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "            (output): BertSelfOutput(\n",
            "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (intermediate): BertIntermediate(\n",
            "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "          )\n",
            "          (output): BertOutput(\n",
            "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "            (dropout): Dropout(p=0.1, inplace=False)\n",
            "          )\n",
            "        )\n",
            "      )\n",
            "    )\n",
            "    (pooler): BertPooler(\n",
            "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "      (activation): Tanh()\n",
            "    )\n",
            "  )\n",
            "  (lstm): LSTM(768, 256, batch_first=True, bidirectional=True)\n",
            "  (out_tag): Linear(in_features=512, out_features=9, bias=True)\n",
            ")\n",
            "final_data {'id': tensor([[  101, 11437,  6105,   188,  7111,  1918,  1110,  1139,  1271,   102,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0]],\n",
            "       device='cuda:0'), 'target_tag': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0]], device='cuda:0'), 'mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0]], device='cuda:0'), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0]], device='cuda:0')}\n",
            "['CLS', 'ni', '##tin', 's', '##har', '##ma', 'is', 'my', 'name', 'SEP']\n",
            "tensor([0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0, 0, 0, 0, 0, 0, 0], device='cuda:0')\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a0uA4e1Gt6WW",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 168
        },
        "outputId": "9fcc032e-669c-4713-b9ce-58aeb91415a8"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'B-LOC ': 5,\n",
              " 'B-MISC ': 2,\n",
              " 'B-ORG ': 0,\n",
              " 'B-PER ': 3,\n",
              " 'I-LOC ': 8,\n",
              " 'I-MISC ': 7,\n",
              " 'I-ORG ': 6,\n",
              " 'I-PER ': 4,\n",
              " 'O ': 1}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 127
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rVmKiI7imbSM"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "drY86OW1u0KH",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "5669bba3-ca88-4ec0-b3c9-3fbc05640038"
      },
      "source": [
        "x['id'].size()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([128])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 53
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sr7tl-pxvQD5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "5506df5a-56ec-4ddb-c369-b73191641713"
      },
      "source": [
        "x['mask'].size()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([128])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 54
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xR1WrXLSvUie",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "f18c3c8b-dd5e-4542-b057-37fb9472de8a"
      },
      "source": [
        "x['token_type_ids'].size()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([128])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 55
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gm9PoB7YvW9G"
      },
      "source": [
        "import torch"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Hk4KteoFqT5_"
      },
      "source": [
        "x=torch.ones(5,20,10)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2HWRk2XmqWB_",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "1168a8d4-511e-471d-dd6c-abcf345e58bf"
      },
      "source": [
        "x.view(-1)==1"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([True, True, True, True, True, True, True, True, True, True, True, True,\n",
              "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
              "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
              "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
              "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
              "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
              "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
              "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
              "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
              "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
              "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
              "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
              "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
              "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
              "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
              "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
              "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
              "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
              "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
              "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
              "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
              "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
              "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
              "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
              "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
              "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
              "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
              "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
              "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
              "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
              "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
              "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
              "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
              "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
              "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
              "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
              "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
              "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
              "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
              "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
              "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
              "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
              "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
              "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
              "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
              "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
              "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
              "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
              "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
              "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
              "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
              "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
              "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
              "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
              "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
              "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
              "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
              "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
              "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
              "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
              "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
              "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
              "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
              "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
              "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
              "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
              "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
              "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
              "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
              "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
              "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
              "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
              "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
              "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
              "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
              "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
              "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
              "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
              "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
              "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
              "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
              "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
              "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
              "        True, True, True, True])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oduQvAbTqhXn"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}